{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The most popular version of the normalization layer is the batch-norm layer.\n",
        "\n",
        "Let's consider its operation in the simplest case, when a batch of one-dimensional vectors is supplied to the input:\n",
        "A batch of one-dimensional vectors is supplied as input:\n",
        "\n",
        "*   A batch of one-dimensional vectors is supplied as input:\n",
        "\n",
        "\n",
        "![](https://ucarecdn.com/c168101b-dc7d-4832-94e2-20de6e43c54f/)\n",
        "\n",
        "where **j** is the vector index inside the batch, **i** is the component number.\n",
        "\n",
        "For the current batch:\n",
        "* For each input component, the expected value and variance are calculated:\n",
        "\n",
        "![](https://ucarecdn.com/e31fefb1-8398-4b7a-91ac-961278493d3e/)\n",
        "![](https://ucarecdn.com/2bd38d22-00c5-4a7d-ac18-2c4fbf8e0ef2/)\n",
        "\n",
        "* The input is normalized by the formula:\n",
        "![](https://ucarecdn.com/dd34accb-876d-46f5-b092-5cd167e704d5/)\n",
        "\n",
        "Epsilon is needed for the zero variance case.\n",
        "\n",
        "* The normalized input is converted as follows:\n",
        "\n",
        "![](https://ucarecdn.com/4ffd57b4-3b7f-4823-87e6-6270c3b24120/)\n",
        "\n",
        "Where  **Gamma** and **Beta** are the learnable parameters of the layer. Please note that Gamma and Beta are vectors of the same length as the input instances.\n",
        "\n",
        "They can be fixed, for example, the simplest case - Beta is assumed to be equal to the zero vector, Gamma - to the vector of ones.\n",
        "\n",
        "If we take Gamma equal to the denominator of the fraction from the formula for Z, and Beta equal to the mathematical expectation, then the layer will return the input tensor unchanged. That is, the layer will be equivalent to the identity function.\n",
        "\n",
        "\n",
        "\n",
        "Thus, the Beta and Gamma parameters make it possible not to lose information entering the layer, and at the same time, the batch norm layer normalizes the input. The latter speeds up the convergence of network parameters, and in some cases it is extremely difficult to achieve network convergence without normalization.\n",
        "\n",
        "The final formula for converting the input is:\n",
        "![](https://ucarecdn.com/e790660f-54c0-4c5e-aa58-4c17ae050594/)\n"
      ],
      "metadata": {
        "id": "KTTR38tLqy_0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problems 1 (5 Points)\n",
        "\n",
        "In this task, you need to implement the batch normalization function without using the [standard function](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d) with the following simplifications:\n",
        "\n",
        "* The Beta parameter is set to 0.\n",
        "* The Gamma parameter is taken equal to 1.\n",
        "* The function should work correctly only during the training phase.\n",
        "* The input has the dimension number of elements in the batch * length of each instance.\n",
        "\n",
        "\n",
        "> Look very carefully at the definition of the [function](https://pytorch.org/docs/stable/torch.html#torch.std) that calculates std."
      ],
      "metadata": {
        "id": "7g-_bfvlvqFT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GEEnCSHqRq3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def custom_batch_norm1d(input_tensor, eps):\n",
        "    mean = torch.mean(input_tensor, dim=0)\n",
        "    var = torch.var(input_tensor, dim=0, unbiased=False)\n",
        "    normed_tensor = (input_tensor - mean) / torch.sqrt(var + eps)\n",
        "    return normed_tensor\n",
        "\n",
        "input_tensor = torch.Tensor([[0.0, 0, 1, 0, 2], [0, 1, 1, 0, 10]])\n",
        "batch_norm = nn.BatchNorm1d(input_tensor.shape[1], affine=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation check"
      ],
      "metadata": {
        "id": "ybma2Pcaw7Gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "all_correct = True\n",
        "for eps_power in range(10):\n",
        "  eps = np.power(10., -eps_power)\n",
        "  batch_norm.eps = eps\n",
        "  batch_norm_out = batch_norm(input_tensor)\n",
        "  custom_batch_norm_out = custom_batch_norm1d(input_tensor, eps)\n",
        "\n",
        "  all_correct &= torch.allclose(batch_norm_out, custom_batch_norm_out)\n",
        "  all_correct &= batch_norm_out.shape == custom_batch_norm_out.shape\n",
        "print(all_correct)"
      ],
      "metadata": {
        "id": "-HcDPtb9w3P4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46031d70-2f07-4a72-8007-fd4b4e9033b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problems 2 (5 Points)\n",
        "Let's generalize the function from the previous step a little - we'll add the ability to set the Beta and Gamma parameters.\n",
        "\n",
        "In this task, you need to implement the batch normalization function without using the [standard function](https://pytorch.org/docs/stable/nn.html#batchnorm1d) with the following simplifications:\n",
        "\n",
        "* The function should work correctly only during the training phase.\n",
        "* The input has the dimension number of elements in the batch * length of each instance."
      ],
      "metadata": {
        "id": "FG5kJiS0xZZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "input_size = 7\n",
        "batch_size = 5\n",
        "input_tensor = torch.randn(batch_size, input_size, dtype=torch.float)\n",
        "\n",
        "eps = 1e-3\n",
        "\n",
        "def custom_batch_norm1d(input_tensor, weight, bias, eps):\n",
        "    mean = torch.mean(input_tensor, dim=0)\n",
        "    var = torch.var(input_tensor, dim=0, unbiased=False)\n",
        "    normed_tensor = (input_tensor - mean) / torch.sqrt(var + eps)\n",
        "    return weight * normed_tensor + bias"
      ],
      "metadata": {
        "id": "1GadO6DpxQYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation check"
      ],
      "metadata": {
        "id": "BEieP3nGzGqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_norm = nn.BatchNorm1d(input_size, eps=eps)\n",
        "batch_norm.bias.data = torch.randn(input_size, dtype=torch.float)\n",
        "batch_norm.weight.data = torch.randn(input_size, dtype=torch.float)\n",
        "batch_norm_out = batch_norm(input_tensor)\n",
        "custom_batch_norm_out = custom_batch_norm1d(input_tensor, batch_norm.weight.data, batch_norm.bias.data, eps)\n",
        "print(torch.allclose(batch_norm_out, custom_batch_norm_out) \\\n",
        "      and batch_norm_out.shape == custom_batch_norm_out.shape)"
      ],
      "metadata": {
        "id": "Un0JQKk7zGDL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea82a026-9561-4d5f-9265-5b887feb4acf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problems 3 (5 Points)\n",
        "\n",
        "Let's get rid of one more simplification - we implement the operation of the batch normalization layer at the prediction stage.\n",
        "\n",
        "At this stage, instead of batch statistics, we will use exponentially smoothed statistics from the layer’s training history.\n",
        "\n",
        "In this step, you need to implement a full-fledged batch normalization class without using a [standard function](https://pytorch.org/docs/stable/nn.html#batchnorm1d) that takes a two-dimensional tensor as input. Be careful, the variance is calculated using a biased sample, and the moving average is calculated using an unbiased sample."
      ],
      "metadata": {
        "id": "GbyRepWSzVjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "input_size = 3\n",
        "batch_size = 5\n",
        "eps = 1e-1\n",
        "\n",
        "\n",
        "class CustomBatchNorm1d:\n",
        "    def __init__(self, weight, bias, eps, momentum):\n",
        "        self.weight = weight\n",
        "        self.bias = bias\n",
        "        self.eps = eps\n",
        "        self.momentum = momentum\n",
        "        self.running_mean = torch.zeros_like(weight)\n",
        "        self.running_var = torch.ones_like(weight)\n",
        "        self.training = True\n",
        "\n",
        "    def __call__(self, input_tensor):\n",
        "        if self.training:\n",
        "            mean = torch.mean(input_tensor, dim=0)\n",
        "            var = torch.var(input_tensor, dim=0, unbiased=False)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean\n",
        "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * torch.var(input_tensor, dim=0, unbiased=True)\n",
        "\n",
        "            normed_tensor = (input_tensor - mean) / torch.sqrt(var + self.eps)\n",
        "        else:\n",
        "            normed_tensor = (input_tensor - self.running_mean) / torch.sqrt(self.running_var + self.eps)\n",
        "\n",
        "        return self.weight * normed_tensor + self.bias\n",
        "\n",
        "    def eval(self):\n",
        "        self.training = False\n",
        "\n",
        "batch_norm = nn.BatchNorm1d(input_size, eps=eps)\n",
        "batch_norm.bias.data = torch.randn(input_size, dtype=torch.float)\n",
        "batch_norm.weight.data = torch.randn(input_size, dtype=torch.float)\n",
        "batch_norm.momentum = 0.5\n",
        "\n",
        "custom_batch_norm1d = CustomBatchNorm1d(batch_norm.weight.data,\n",
        "                                        batch_norm.bias.data, eps, batch_norm.momentum)"
      ],
      "metadata": {
        "id": "MQurnWQXz4f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation check"
      ],
      "metadata": {
        "id": "_-eW16Km0sKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_correct = True\n",
        "\n",
        "for i in range(8):\n",
        "  torch_input = torch.randn(batch_size, input_size, dtype=torch.float)\n",
        "  norm_output = batch_norm(torch_input)\n",
        "  custom_output = custom_batch_norm1d(torch_input)\n",
        "  all_correct &= torch.allclose(norm_output, custom_output, atol=1e-04) \\\n",
        "  and norm_output.shape == custom_output.shape\n",
        "\n",
        "  batch_norm.eval()\n",
        "  custom_batch_norm1d.eval()\n",
        "\n",
        "for i in range(8):\n",
        "  torch_input = torch.randn(batch_size, input_size, dtype=torch.float)\n",
        "  norm_output = batch_norm(torch_input)\n",
        "  custom_output = custom_batch_norm1d(torch_input)\n",
        "  all_correct &= torch.allclose(norm_output, custom_output, atol=1e-04) \\\n",
        "  and norm_output.shape == custom_output.shape\n",
        "\n",
        "print(all_correct)"
      ],
      "metadata": {
        "id": "_FHumpT40uDi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3718e375-6962-4d72-dc4b-ca732dc107a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problems 4 (5 Points)\n",
        "\n",
        "As you can see, implementing the batch norm layer at the prediction stage is not so easy, so in the later steps of this workshop we will no longer require implementing this part.\n",
        "\n",
        "A batch normalization layer exists for input of any dimension.\n",
        "\n",
        "In this step we will consider it for input from multi-channel two-dimensional tensors, for example, images.\n",
        "\n",
        "If you extract each channel of the image into a vector, then the input will be three-dimensional:\n",
        "\n",
        "* number of pictures in the batch\n",
        "* number of channels in each picture\n",
        "* number of pixels in the image\n",
        "\n",
        "![](https://ucarecdn.com/2ce27998-abb8-4888-9034-97fe4efe95ef/)\n",
        "\n",
        "Normalization process:\n",
        "\n",
        "* The input is divided into slices parallel to the blue part. That is, each slice is all the pixels of all images in one of the channels.\n",
        "* For each cut, a mat is considered. expectation and variance.\n",
        "* Each slice is normalized independently.\n",
        "\n",
        "At this step, you are asked to **implement a batch norm layer for a four-dimensional input** (for example, a batch of multi-channel two-dimensional images) without using the [standard function](https://pytorch.org/docs/stable/nn.html#batchnorm2d) with the following simplifications:\n",
        "\n",
        "* Beta parameter = 0.\n",
        "* Gamma parameter = 1.\n",
        "* The function should work correctly only during the training phase."
      ],
      "metadata": {
        "id": "mStfgfEF1I-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "eps = 1e-3\n",
        "\n",
        "input_channels = 3\n",
        "batch_size = 3\n",
        "height = 10\n",
        "width = 10\n",
        "\n",
        "batch_norm_2d = nn.BatchNorm2d(input_channels, affine=False, eps=eps)\n",
        "\n",
        "input_tensor = torch.randn(batch_size, input_channels, height, width, dtype=torch.float)\n",
        "\n",
        "\n",
        "def custom_batch_norm2d(input_tensor, eps):\n",
        "    mean = torch.mean(input_tensor, dim=(0, 2, 3), keepdim=True)\n",
        "    var = torch.var(input_tensor, dim=(0, 2, 3), unbiased=False, keepdim=True)\n",
        "\n",
        "    normed_tensor = (input_tensor - mean) / torch.sqrt(var + eps)\n",
        "    return normed_tensor"
      ],
      "metadata": {
        "id": "GOGUsKH43BQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation check"
      ],
      "metadata": {
        "id": "bR5rdo-v3uft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norm_output = batch_norm_2d(input_tensor)\n",
        "custom_output = custom_batch_norm2d(input_tensor, eps)\n",
        "print(torch.allclose(norm_output, custom_output) and norm_output.shape == custom_output.shape)"
      ],
      "metadata": {
        "id": "vFQCKJZx3twp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "644af2a3-b205-4a52-841b-a8938d09ecc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We took a closer look at batch-by-batch normalization. To simplify further presentation, we will focus on the case of a three-dimensional tensor at the input of the layer; if the input dimension is more than three, then we will extend all dimensions except the first two into one dimension.\n",
        "\n",
        "There is normalization not only by batch, but also by other dimensions.\n",
        "\n",
        "Take a look at the images below.\n",
        "\n",
        "![](https://ucarecdn.com/d1894e62-5608-43ce-80a0-f767d1875ff9/)\n",
        "\n",
        "Where:\n",
        "\n",
        "* C - number of input channels.\n",
        "* N - batch size.\n",
        "* H, W - dimension according to the last (third) dimension of the input.\n",
        "\n",
        "\n",
        "The following types of normalization can be seen in the image:\n",
        "\n",
        "* By batch.\n",
        "* By channel.\n",
        "* By instance.\n",
        "* By group.\n",
        "\n",
        "In addition to these types, there are also many others that are beyond the scope of our cource.\n",
        "\n",
        "We will consider these types of normalization in further steps."
      ],
      "metadata": {
        "id": "5R_jUvGH4UeU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problems 5 (5 Points)\n",
        "\n",
        "The idea behind the per-channel normalization layer is that the network should be independent of the contrast of the original image.\n",
        "\n",
        "Channel normalization works independently for each batch image.\n",
        "\n",
        "![](https://ucarecdn.com/c9f3f179-7f3d-44dc-85ef-8d1a675dc6c4/)\n",
        "\n",
        "This step asks you to implement per-channel normalization without using a [standard layer](https://pytorch.org/docs/stable/nn.html#layernorm), with the following simplifications:\n",
        "\n",
        "* Beta parameter = 0.\n",
        "* Gamma parameter = 1.\n",
        "* Only the training phase needs to be implemented.\n",
        "* Normalization is done across all input dimensions except zero.\n",
        "\n",
        "Please note that the input dimension is not fixed at this step.\n",
        "\n",
        "Let us clarify that in the “by channel” normalization layer, statistics are calculated for all dimensions except zero."
      ],
      "metadata": {
        "id": "CN3xWCSC38QJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "eps = 1e-10\n",
        "\n",
        "\n",
        "def custom_layer_norm(input_tensor, eps):\n",
        "    normalize_dims = tuple(range(1, input_tensor.dim()))\n",
        "    mean = torch.mean(input_tensor, dim=normalize_dims, keepdim=True)\n",
        "    var = torch.var(input_tensor, dim=normalize_dims, unbiased=False, keepdim=True)\n",
        "\n",
        "    normed_tensor = (input_tensor - mean) / torch.sqrt(var + eps)\n",
        "    return normed_tensor"
      ],
      "metadata": {
        "id": "Vr5yOvn45gMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation check"
      ],
      "metadata": {
        "id": "BPPWIaXj7Bl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_correct = True\n",
        "for dim_count in range(3, 9):\n",
        "  input_tensor = torch.randn(*list(range(3, dim_count + 2)), dtype=torch.float)\n",
        "  layer_norm = nn.LayerNorm(input_tensor.size()[1:], elementwise_affine=False, eps=eps)\n",
        "\n",
        "  norm_output = layer_norm(input_tensor)\n",
        "  custom_output = custom_layer_norm(input_tensor, eps)\n",
        "\n",
        "  all_correct &= torch.allclose(norm_output, custom_output, 1e-2)\n",
        "  all_correct &= norm_output.shape == custom_output.shape\n",
        "print(all_correct)"
      ],
      "metadata": {
        "id": "LTzirTsu7Dy8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebdddc22-4731-428c-d017-597299344113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problems 6 (5 Points)\n",
        "\n",
        "Instance normalization was originally developed for the style transfer task. The idea behind this layer is that the network should be independent of the contrast of the individual channels of the source image.\n",
        "\n",
        "![](https://ucarecdn.com/fe13e8df-e2f8-4356-9001-8dc3cd734e64/)\n",
        "\n",
        "This step asks you to implement per-instance normalization without using a standard layer with the following simplifications:\n",
        "\n",
        "* Beta parameter = 0.\n",
        "* Gamma parameter = 1.\n",
        "* The input is a three-dimensional tensor (batch size, number of channels, length of each instance channel).\n",
        "* Only the training phase needs to be implemented.\n",
        "\n",
        "In the “by instance” normalization layer, statistics are calculated according to the last dimension (for each input channel of each input example).\n",
        "\n"
      ],
      "metadata": {
        "id": "m4KXCCxN7jPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "eps = 1e-3\n",
        "\n",
        "batch_size = 5\n",
        "input_channels = 2\n",
        "input_length = 30\n",
        "\n",
        "instance_norm = nn.InstanceNorm1d(input_channels, affine=False, eps=eps)\n",
        "\n",
        "input_tensor = torch.randn(batch_size, input_channels, input_length, dtype=torch.float)\n",
        "\n",
        "\n",
        "def custom_instance_norm1d(input_tensor, eps):\n",
        "    mean = torch.mean(input_tensor, dim=2, keepdim=True)\n",
        "    var = torch.var(input_tensor, dim=2, unbiased=False, keepdim=True)\n",
        "\n",
        "    normed_tensor = (input_tensor - mean) / torch.sqrt(var + eps)\n",
        "    return normed_tensor"
      ],
      "metadata": {
        "id": "RXGfuvLr8Mtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation check"
      ],
      "metadata": {
        "id": "hRNtrMHF8Qz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "norm_output = instance_norm(input_tensor)\n",
        "custom_output = custom_instance_norm1d(input_tensor, eps)\n",
        "print(torch.allclose(norm_output, custom_output, atol=1e-06) and norm_output.shape == custom_output.shape)"
      ],
      "metadata": {
        "id": "XxT16aS98QCW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a137b809-485f-4b08-f443-55386e101b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problems 7 (5 Points)\n",
        "\n",
        "Per-group normalization is a generalization of per-channel and per-instance normalization.\n",
        "\n",
        "The channels in an image are not completely independent, so the ability to use neighboring channel statistics is an advantage of by-group normalization over by-instance normalization.\n",
        "\n",
        "At the same time, the image channels can vary greatly, so group normalization is more flexible than per channel normalization.\n",
        "\n",
        "![](https://ucarecdn.com/7384f3ed-ac36-48dc-8b70-6fdc490f5092/)\n",
        "\n",
        "This step asks you to implement \"by group\" normalization without using a [standard layer](https://pytorch.org/docs/stable/nn.html#groupnorm) with the following simplifications:\n",
        "\n",
        "* Beta parameter = 0.\n",
        "* Gamma parameter = 1.\n",
        "* Only the training phase needs to be implemented.\n",
        "* A three-dimensional tensor is supplied as input.\n",
        "\n",
        "The layer also takes the number of groups as input.\n",
        "\n",
        "In the “by group” normalization layer, statistics are calculated very similarly to “by channel” normalization, only the channels are divided into groups."
      ],
      "metadata": {
        "id": "mOykZEPF8X2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation check"
      ],
      "metadata": {
        "id": "xa3gyLEF-dYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "channel_count = 6\n",
        "eps = 1e-3\n",
        "batch_size = 20\n",
        "input_size = 2\n",
        "\n",
        "input_tensor = torch.randn(batch_size, channel_count, input_size)\n",
        "\n",
        "\n",
        "def custom_group_norm(input_tensor, groups, eps):\n",
        "    N, C, L = input_tensor.shape\n",
        "    reshaped_input = input_tensor.view(N, groups, C // groups, L)\n",
        "\n",
        "    mean = torch.mean(reshaped_input, dim=(2, 3), keepdim=True)\n",
        "    var = torch.var(reshaped_input, dim=(2, 3), unbiased=False, keepdim=True)\n",
        "\n",
        "    normed_tensor = (reshaped_input - mean) / torch.sqrt(var + eps)\n",
        "    normed_tensor = normed_tensor.view(N, C, L)\n",
        "\n",
        "    return normed_tensor"
      ],
      "metadata": {
        "id": "8htXNSHO-Nmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_correct = True\n",
        "for groups in [1, 2, 3, 6]:\n",
        "  group_norm = nn.GroupNorm(groups, channel_count, eps=eps, affine=False)\n",
        "  norm_output = group_norm(input_tensor)\n",
        "  custom_output = custom_group_norm(input_tensor, groups, eps)\n",
        "  all_correct &= torch.allclose(norm_output, custom_output, 1e-3)\n",
        "  all_correct &= norm_output.shape == custom_output.shape\n",
        "print(all_correct)"
      ],
      "metadata": {
        "id": "JlHYhkFH-S98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f9cf88e-5fae-42c3-c358-1ede6a7277ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    }
  ]
}