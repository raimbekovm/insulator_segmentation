{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insulator Segmentation with U-Net + ResNet34\n",
    "\n",
    "This notebook implements semantic segmentation for insulator detection in power line images.\n",
    "\n",
    "**Architecture**: U-Net with pretrained ResNet34 encoder  \n",
    "**Expected Performance**: 0.88-0.92 Dice coefficient  \n",
    "**Training Time**: ~13 hours on 2Ã—Tesla T4 GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'GPU Count: {torch.cuda.device_count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    DATA_DIR = Path('data/train')\n",
    "    TEST_DIR = Path('data/test')\n",
    "    WORK_DIR = Path('outputs')\n",
    "    CHECKPOINT_DIR = WORK_DIR / 'checkpoints'\n",
    "    PREDICTION_DIR = WORK_DIR / 'predictions'\n",
    "    \n",
    "    CHECKPOINT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    PREDICTION_DIR.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    ENCODER = 'resnet34'\n",
    "    IMG_SIZE = 512\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_EPOCHS = 40\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    NUM_WORKERS = 2\n",
    "    VAL_SPLIT = 0.15\n",
    "    EARLY_STOPPING_PATIENCE = 7\n",
    "    USE_TTA = True\n",
    "    OPTIMIZE_THRESHOLD = True\n",
    "    SAVE_EVERY_N_EPOCHS = 5\n",
    "\n",
    "cfg = Config()\n",
    "print(f'Configuration: {cfg.NUM_EPOCHS} epochs, batch size {cfg.BATCH_SIZE}, TTA={cfg.USE_TTA}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNetResNet(nn.Module):\n",
    "    def __init__(self, encoder_name='resnet34', pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        import torchvision.models as models\n",
    "        if encoder_name == 'resnet34':\n",
    "            resnet = models.resnet34(pretrained=pretrained)\n",
    "        else:\n",
    "            resnet = models.resnet50(pretrained=pretrained)\n",
    "        \n",
    "        self.enc0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu)\n",
    "        self.enc1 = nn.Sequential(resnet.maxpool, resnet.layer1)\n",
    "        self.enc2 = resnet.layer2\n",
    "        self.enc3 = resnet.layer3\n",
    "        self.enc4 = resnet.layer4\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.conv1 = ConvBlock(512, 256)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.conv2 = ConvBlock(256, 128)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.conv3 = ConvBlock(128, 64)\n",
    "        \n",
    "        self.up4 = nn.ConvTranspose2d(64, 64, 2, stride=2)\n",
    "        self.conv4 = ConvBlock(128, 64)\n",
    "        \n",
    "        self.up5 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n",
    "        self.conv5 = ConvBlock(32, 32)\n",
    "        \n",
    "        self.final = nn.Conv2d(32, 1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        e0 = self.enc0(x)\n",
    "        e1 = self.enc1(e0)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        e4 = self.enc4(e3)\n",
    "        \n",
    "        x = self.up1(e4)\n",
    "        x = torch.cat([x, e3], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        \n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, e2], dim=1)\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x, e1], dim=1)\n",
    "        x = self.conv3(x)\n",
    "        \n",
    "        x = self.up4(x)\n",
    "        x = torch.cat([x, e0], dim=1)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        x = self.up5(x)\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        return self.final(x)\n",
    "\n",
    "print('Model architecture defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_paths(data_dir, test_dir):\n",
    "    train_img_dir = data_dir / 'images'\n",
    "    train_mask_dir = data_dir / 'masks'\n",
    "    test_img_dir = test_dir / 'images'\n",
    "    \n",
    "    train_images = sorted(list(train_img_dir.glob('*.jpg')))\n",
    "    train_masks = sorted(list(train_mask_dir.glob('*.png')))\n",
    "    test_images = sorted(list(test_img_dir.glob('*.jpg')))\n",
    "    \n",
    "    print(f'Train: {len(train_images)} images, {len(train_masks)} masks')\n",
    "    print(f'Test: {len(test_images)} images')\n",
    "    \n",
    "    assert len(train_images) == len(train_masks), 'Mismatch between images and masks'\n",
    "    return train_images, train_masks, test_images\n",
    "\n",
    "train_images, train_masks, test_images = get_file_paths(cfg.DATA_DIR, cfg.TEST_DIR)\n",
    "\n",
    "train_imgs, val_imgs, train_msks, val_msks = train_test_split(\n",
    "    train_images, train_masks, test_size=cfg.VAL_SPLIT, random_state=42\n",
    ")\n",
    "\n",
    "print(f'Split: {len(train_imgs)} train, {len(val_imgs)} val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms(img_size=512):\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=45, p=0.6),\n",
    "        A.OneOf([A.RandomBrightnessContrast(p=1), A.HueSaturationValue(p=1)], p=0.5),\n",
    "        A.OneOf([A.GaussNoise(p=1), A.GaussianBlur(p=1)], p=0.3),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_val_transforms(img_size=512):\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "print('Augmentation pipelines defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsulatorDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths=None, transforms=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transforms = transforms\n",
    "        self.is_test = mask_paths is None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.is_test:\n",
    "            if self.transforms:\n",
    "                image = self.transforms(image=image)['image']\n",
    "            return image, str(img_path.name)\n",
    "        else:\n",
    "            mask_path = self.mask_paths[idx]\n",
    "            mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "            mask = (mask > 127).astype(np.float32)\n",
    "            \n",
    "            if self.transforms:\n",
    "                augmented = self.transforms(image=image, mask=mask)\n",
    "                image = augmented['image']\n",
    "                mask = augmented['mask']\n",
    "            \n",
    "            mask = mask.unsqueeze(0)\n",
    "            return image, mask\n",
    "\n",
    "train_dataset = InsulatorDataset(train_imgs, train_msks, get_train_transforms(cfg.IMG_SIZE))\n",
    "val_dataset = InsulatorDataset(val_imgs, val_msks, get_val_transforms(cfg.IMG_SIZE))\n",
    "test_dataset = InsulatorDataset(test_images, None, get_val_transforms(cfg.IMG_SIZE))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, \n",
    "                         num_workers=cfg.NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False, \n",
    "                       num_workers=cfg.NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False, \n",
    "                        num_workers=cfg.NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f'DataLoaders created: {len(train_loader)} train batches, {len(val_loader)} val batches, {len(test_loader)} test batches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Loss Function and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNetResNet(encoder_name=cfg.ENCODER, pretrained=True)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f'Using {torch.cuda.device_count()} GPUs')\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        bce = self.bce(logits, targets)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        intersection = (probs * targets).sum(dim=(2, 3))\n",
    "        union = probs.sum(dim=(2, 3)) + targets.sum(dim=(2, 3))\n",
    "        dice = (2. * intersection + 1e-6) / (union + 1e-6)\n",
    "        return 0.5 * bce + 0.5 * (1 - dice.mean())\n",
    "\n",
    "def dice_coefficient(pred, target, threshold=0.5):\n",
    "    pred = (pred > threshold).float()\n",
    "    intersection = (pred * target).sum(dim=(2, 3))\n",
    "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
    "    return ((2. * intersection + 1e-6) / (union + 1e-6)).mean()\n",
    "\n",
    "criterion = DiceBCELoss()\n",
    "print('Model and loss function initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, scaler, device):\n",
    "    model.train()\n",
    "    total_loss, total_dice = 0, 0\n",
    "    \n",
    "    for images, masks in tqdm(loader, desc='Training'):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, masks)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            dice = dice_coefficient(torch.sigmoid(logits), masks)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_dice += dice.item()\n",
    "    \n",
    "    return total_loss / len(loader), total_dice / len(loader)\n",
    "\n",
    "def validate_epoch(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_dice = 0, 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(loader, desc='Validation'):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, masks)\n",
    "            dice = dice_coefficient(torch.sigmoid(logits), masks)\n",
    "            total_loss += loss.item()\n",
    "            total_dice += dice.item()\n",
    "    \n",
    "    return total_loss / len(loader), total_dice / len(loader)\n",
    "\n",
    "print('Training functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Checkpoint Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointManager:\n",
    "    def __init__(self, checkpoint_dir):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.best_score = -np.inf\n",
    "    \n",
    "    def save(self, model, optimizer, scaler, epoch, metrics, is_best=False):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.module.state_dict() if hasattr(model, 'module') else model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scaler_state_dict': scaler.state_dict(),\n",
    "            'metrics': metrics,\n",
    "        }\n",
    "        torch.save(checkpoint, self.checkpoint_dir / 'latest.pth')\n",
    "        if is_best:\n",
    "            torch.save(checkpoint, self.checkpoint_dir / 'best.pth')\n",
    "            self.best_score = metrics['val_dice']\n",
    "            print(f\"Saved best model: {metrics['val_dice']:.4f}\")\n",
    "    \n",
    "    def load(self, model, optimizer=None, scaler=None, name='best.pth'):\n",
    "        path = self.checkpoint_dir / name\n",
    "        if not path.exists():\n",
    "            return None\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        if hasattr(model, 'module'):\n",
    "            model.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        if optimizer:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        if scaler:\n",
    "            scaler.load_state_dict(checkpoint['scaler_state_dict'])\n",
    "        print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
    "        return checkpoint\n",
    "\n",
    "ckpt_mgr = CheckpointManager(cfg.CHECKPOINT_DIR)\n",
    "print('Checkpoint manager initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=cfg.LEARNING_RATE, weight_decay=cfg.WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=15, T_mult=2, eta_min=1e-6)\n",
    "scaler = GradScaler()\n",
    "\n",
    "history = {'train_loss': [], 'train_dice': [], 'val_loss': [], 'val_dice': []}\n",
    "best_val_dice, patience = -np.inf, 0\n",
    "\n",
    "print('Starting training...')\n",
    "\n",
    "for epoch in range(1, cfg.NUM_EPOCHS + 1):\n",
    "    print(f'\\nEpoch {epoch}/{cfg.NUM_EPOCHS}')\n",
    "    \n",
    "    train_loss, train_dice = train_epoch(model, train_loader, criterion, optimizer, scaler, device)\n",
    "    val_loss, val_dice = validate_epoch(model, val_loader, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_dice'].append(train_dice)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_dice'].append(val_dice)\n",
    "    \n",
    "    print(f'Train - Loss: {train_loss:.4f}, Dice: {train_dice:.4f}')\n",
    "    print(f'Val   - Loss: {val_loss:.4f}, Dice: {val_dice:.4f}')\n",
    "    \n",
    "    is_best = val_dice > best_val_dice\n",
    "    \n",
    "    if epoch % cfg.SAVE_EVERY_N_EPOCHS == 0 or is_best:\n",
    "        ckpt_mgr.save(model, optimizer, scaler, epoch, \n",
    "                     {'train_loss': train_loss, 'train_dice': train_dice,\n",
    "                      'val_loss': val_loss, 'val_dice': val_dice}, is_best)\n",
    "    \n",
    "    if is_best:\n",
    "        best_val_dice = val_dice\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= cfg.EARLY_STOPPING_PATIENCE:\n",
    "            print(f'Early stopping triggered after {epoch} epochs')\n",
    "            break\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f'\\nTraining complete. Best validation Dice: {best_val_dice:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Threshold Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.OPTIMIZE_THRESHOLD:\n",
    "    print('Optimizing threshold...')\n",
    "    ckpt_mgr.load(model, name='best.pth')\n",
    "    model.eval()\n",
    "    \n",
    "    best_threshold, best_score = 0.5, 0\n",
    "    \n",
    "    for thresh in np.arange(0.3, 0.7, 0.05):\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                probs = torch.sigmoid(model(images))\n",
    "                total += dice_coefficient(probs, masks, thresh).item()\n",
    "        \n",
    "        avg = total / len(val_loader)\n",
    "        print(f'Threshold {thresh:.2f}: Dice = {avg:.4f}')\n",
    "        \n",
    "        if avg > best_score:\n",
    "            best_score = avg\n",
    "            best_threshold = thresh\n",
    "    \n",
    "    print(f'\\nBest threshold: {best_threshold:.2f} (Dice: {best_score:.4f})')\n",
    "else:\n",
    "    best_threshold = 0.5\n",
    "    print(f'Using default threshold: {best_threshold}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Generate Test Predictions with TTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_tta(model, image, device):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred1 = torch.sigmoid(model(image.to(device)))\n",
    "        pred2 = torch.sigmoid(model(torch.flip(image, [3]).to(device)))\n",
    "        pred2 = torch.flip(pred2, [3])\n",
    "        pred3 = torch.sigmoid(model(torch.flip(image, [2]).to(device)))\n",
    "        pred3 = torch.flip(pred3, [2])\n",
    "        pred4 = torch.sigmoid(model(torch.flip(image, [2, 3]).to(device)))\n",
    "        pred4 = torch.flip(pred4, [2, 3])\n",
    "    \n",
    "    return (pred1 + pred2 + pred3 + pred4) / 4\n",
    "\n",
    "ckpt_mgr.load(model, name='best.pth')\n",
    "model.eval()\n",
    "\n",
    "print('Generating test predictions...')\n",
    "\n",
    "for images, filenames in tqdm(test_loader, desc='Predicting'):\n",
    "    if cfg.USE_TTA:\n",
    "        preds = predict_with_tta(model, images, device)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(model(images.to(device)))\n",
    "    \n",
    "    preds = (preds.cpu().numpy() > best_threshold).astype(np.uint8)\n",
    "    \n",
    "    for pred, filename in zip(preds, filenames):\n",
    "        mask = pred[0]\n",
    "        output_path = cfg.PREDICTION_DIR / filename.replace('.jpg', '.png')\n",
    "        cv2.imwrite(str(output_path), mask * 255)\n",
    "\n",
    "print('Predictions saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Create Submission Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "print('Creating submission archive...')\n",
    "shutil.make_archive(str(cfg.WORK_DIR / 'submission'), 'zip', cfg.PREDICTION_DIR)\n",
    "\n",
    "print('\\nTraining Summary:')\n",
    "print(f'Best Validation Dice: {best_val_dice:.4f}')\n",
    "print(f'Optimal Threshold: {best_threshold:.2f}')\n",
    "print(f'Submission file: {cfg.WORK_DIR / \"submission.zip\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_mgr.load(model, name='best.pth')\n",
    "model.eval()\n",
    "\n",
    "test_img = test_images[0]\n",
    "image = cv2.cvtColor(cv2.imread(str(test_img)), cv2.COLOR_BGR2RGB)\n",
    "aug = get_val_transforms(cfg.IMG_SIZE)(image=image)\n",
    "img_t = aug['image'].unsqueeze(0).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = torch.sigmoid(model(img_t)).cpu().numpy()[0, 0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title('Original Image')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(pred, cmap='hot', vmin=0, vmax=1)\n",
    "axes[1].set_title('Prediction Probabilities')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(pred > best_threshold, cmap='gray')\n",
    "axes[2].set_title(f'Binary Mask (threshold={best_threshold:.2f})')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(cfg.WORK_DIR / 'prediction_example.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
